{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg9tS_QK0BDV",
        "outputId": "4e890e66-db7c-48b3-9c62-7143eedfc50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hgdHIdagHm2",
        "outputId": "92fb916c-3d6f-4dbb-a83d-6ea6094ffe31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMlO77uJnvf_",
        "outputId": "c3f13523-c7dd-40e2-f63d-7fd52718d4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying ZIP from Drive to Colab...\n",
            "Source: /content/drive/MyDrive/TKPR221/traffic_signs_dataset.zip\n",
            "Destination: /content/temp/dataset.zip\n",
            "ZIP copied successfully! Size: 126.81 MB\n",
            "\n",
            "Extracting ZIP to: /content/dataset\n",
            "This might take a while...\n",
            "\n",
            "Cleaning up temporary files...\n",
            "\n",
            "Verifying extracted files:\n",
            "Crops directory exists: False\n",
            "Dataset CSV exists: False\n",
            "\n",
            "Dataset extracted successfully to: /content/dataset/crops\n"
          ]
        }
      ],
      "source": [
        "#Assisted by ChaGPT\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "def extract_drive_dataset(\n",
        "    zip_path='/content/drive/MyDrive/TKPR221/traffic_signs_dataset.zip',\n",
        "    extract_path='/content/dataset'\n",
        "):\n",
        "\n",
        "    zip_path = Path(zip_path)\n",
        "    if not zip_path.exists():\n",
        "        raise FileNotFoundError(f\"ZIP file not found at: {zip_path}\")\n",
        "\n",
        "    temp_dir = Path('/content/temp')\n",
        "    temp_dir.mkdir(exist_ok=True)\n",
        "    temp_zip = temp_dir / 'dataset.zip'\n",
        "\n",
        "    print(f\"\\nCopying ZIP from Drive to Colab...\")\n",
        "    print(f\"Source: {zip_path}\")\n",
        "    print(f\"Destination: {temp_zip}\")\n",
        "    shutil.copy2(zip_path, temp_zip)\n",
        "\n",
        "    if temp_zip.exists():\n",
        "        zip_size = temp_zip.stat().st_size / (1024 * 1024)\n",
        "        print(f\"ZIP copied successfully! Size: {zip_size:.2f} MB\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Failed to copy ZIP file\")\n",
        "\n",
        "    extract_dir = Path(extract_path)\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nExtracting ZIP to: {extract_dir}\")\n",
        "    print(\"This might take a while...\")\n",
        "    shutil.unpack_archive(str(temp_zip), str(extract_dir))\n",
        "\n",
        "    print(\"\\nCleaning up temporary files...\")\n",
        "    temp_zip.unlink()\n",
        "    temp_dir.rmdir()\n",
        "\n",
        "    crops_dir = extract_dir / 'crops'\n",
        "    csv_file = crops_dir / 'dataset.csv'\n",
        "\n",
        "    print(\"\\nVerifying extracted files:\")\n",
        "    print(f\"Crops directory exists: {crops_dir.exists()}\")\n",
        "    print(f\"Dataset CSV exists: {csv_file.exists()}\")\n",
        "\n",
        "    if crops_dir.exists():\n",
        "        batch_folders = list(crops_dir.glob('batch_*'))\n",
        "        print(f\"Number of batch folders found: {len(batch_folders)}\")\n",
        "\n",
        "\n",
        "        total_images = sum(len(list(folder.glob('*.jpg'))) for folder in batch_folders)\n",
        "        print(f\"Total number of images found: {total_images}\")\n",
        "\n",
        "\n",
        "        print(\"\\nFolder structure:\")\n",
        "        print(f\"└── {crops_dir.name}/\")\n",
        "        print(f\"    ├── dataset.csv\")\n",
        "        for i, batch in enumerate(sorted(batch_folders)):\n",
        "            is_last = i == len(batch_folders) - 1\n",
        "            prefix = \"    └── \" if is_last else \"    ├── \"\n",
        "            n_images = len(list(batch.glob('*.jpg')))\n",
        "            print(f\"{prefix}{batch.name}/ ({n_images} images)\")\n",
        "\n",
        "    return str(crops_dir)\n",
        "\n",
        "\n",
        "try:\n",
        "    dataset_dir = extract_drive_dataset()\n",
        "    print(f\"\\nDataset extracted successfully to: {dataset_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pK5CV0g70BDX",
        "outputId": "e904e740-6eed-492b-c33c-cb6447554ecc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\njson_data = json.load(open('/content/drive/TKPR221/crops/dataset.json'))\\ncsv_data = []\\nfor image_id, crops in json_data.items():\\n    for crop in crops:\\n        csv_data.append({\\n            'filename': crop['filename'],\\n            'label': crop['label']\\n        })\\n\\ndf = pd.DataFrame(csv_data)\\ndf.to_csv('dataset.csv', index=False)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "json_data = json.load(open('/content/drive/TKPR221/crops/dataset.json'))\n",
        "csv_data = []\n",
        "for image_id, crops in json_data.items():\n",
        "    for crop in crops:\n",
        "        csv_data.append({\n",
        "            'filename': crop['filename'],\n",
        "            'label': crop['label']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(csv_data)\n",
        "df.to_csv('dataset.csv', index=False)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0eJKmH_F0BDY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TrafficSignDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, csv, transformations=None, size=None, flatten=False, label_encoder=None):\n",
        "        self.image_dir = Path(image_dir)\n",
        "        self.csv = csv\n",
        "        self.transformations = transformations\n",
        "        self.size = size\n",
        "        self.flatten = flatten\n",
        "\n",
        "        if label_encoder is None:\n",
        "            self.label_encoder = LabelEncoder()\n",
        "            self.csv['label'] = self.label_encoder.fit_transform(self.csv['label'])\n",
        "        else:\n",
        "            self.label_encoder = label_encoder\n",
        "            self.csv['label'] = self.label_encoder.transform(self.csv['label'])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_data = self.csv.iloc[idx]\n",
        "        image = cv2.imread(str(self.image_dir / image_data['filename']), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if self.transformations:\n",
        "            transformed = self.transformations(image=image)\n",
        "            image = transformed['image']\n",
        "\n",
        "        if self.size is not None:\n",
        "            image = self.resize_with_pad(image, (self.size, self.size))\n",
        "\n",
        "        if self.flatten:\n",
        "            image = image.flatten()\n",
        "\n",
        "        return image, image_data['label']\n",
        "\n",
        "    def transform(self, image):\n",
        "        return self.transformations(image)\n",
        "\n",
        "\n",
        "    def resize_with_pad(self,image: np.array,\n",
        "                    new_shape: tuple[int, int],\n",
        "                    padding_color: tuple[int, ...] = (255, 255, 255)) -> np.array:  # Fixed type hint\n",
        "\n",
        "        ## COPIED FROM : https://gist.github.com/IdeaKing/11cf5e146d23c5bb219ba3508cca89ec\n",
        "\n",
        "        original_shape = (image.shape[1], image.shape[0])\n",
        "        ratio = float(max(new_shape))/max(original_shape)\n",
        "        new_size = tuple([int(x*ratio) for x in original_shape])\n",
        "        image = cv2.resize(image, new_size)\n",
        "        delta_w = new_shape[0] - new_size[0]\n",
        "        delta_h = new_shape[1] - new_size[1]\n",
        "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "        image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=padding_color)\n",
        "        return image\n",
        "\n",
        "    def get_label_encoded_classes(self):\n",
        "        classes = self.label_encoder.classes_\n",
        "        encoded_classes = self.label_encoder.transform(classes)\n",
        "        return encoded_classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "19p7KSCjEI1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681c2ec2-438c-4b8d-ce61-81dfb265bed3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune\n",
            "  Downloading neptune-1.13.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.1.44)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (11.1.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune) (2.10.1)\n",
            "Collecting boto3>=1.28.0 (from neptune)\n",
            "  Downloading boto3-1.35.97-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting bravado<12.0.0,>=11.0.0 (from neptune)\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.8)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.0.0)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.17.0)\n",
            "Collecting swagger-spec-validator>=2.7.4 (from neptune)\n",
            "  Downloading swagger_spec_validator-3.0.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (4.12.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.3.0)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.8.0)\n",
            "Collecting botocore<1.36.0,>=1.35.97 (from boto3>=1.28.0->neptune)\n",
            "  Downloading botocore-1.35.97-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.0->neptune)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.0->neptune)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading bravado-core-6.1.1.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.2)\n",
            "Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2024.12.14)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.23.0)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2024.2)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.22.3)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (3.0.0)\n",
            "Collecting rfc3339-validator (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>0.1.0 (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading neptune-1.13.0-py3-none-any.whl (502 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.97-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Downloading swagger_spec_validator-3.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Downloading botocore-1.35.97-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: bravado-core\n",
            "  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bravado-core: filename=bravado_core-6.1.1-py2.py3-none-any.whl size=67675 sha256=f3d3bc91b12938714470fd7410ecedd4c3a485c4f74aea8f05ab0229cbca52da\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/35/4a/44ec4c358db21a5d63ed4e40f0f0012a438106f220bce4ccba\n",
            "Successfully built bravado-core\n",
            "Installing collected packages: monotonic, uri-template, types-python-dateutil, simplejson, rfc3986-validator, rfc3339-validator, jsonref, jmespath, fqdn, botocore, arrow, s3transfer, isoduration, swagger-spec-validator, boto3, bravado-core, bravado, neptune\n",
            "Successfully installed arrow-1.3.0 boto3-1.35.97 botocore-1.35.97 bravado-11.0.3 bravado-core-6.1.1 fqdn-1.5.1 isoduration-20.11.0 jmespath-1.0.1 jsonref-1.1.0 monotonic-1.6 neptune-1.13.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 s3transfer-0.10.4 simplejson-3.19.3 swagger-spec-validator-3.0.4 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install neptune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8PMPAgf0BDZ",
        "outputId": "a7f54a30-501e-4e4b-9fee-0670e7250df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    filename       label\n",
            "count                                  59534       59534\n",
            "unique                                 59534         399\n",
            "top     batch_5/--FValmNpFJ8yo8X7uWODA_0.jpg  other-sign\n",
            "freq                                       1       39121\n",
            "                                    filename                             label\n",
            "count                                   9975                              9975\n",
            "unique                                  6130                               399\n",
            "top     batch_0/HTi48u68aWbnKR1R6Y7kew_5.jpg  complementary--accident-area--g3\n",
            "freq                                      25                                25\n",
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/muguryalcin/TKPR221/e/TKPR-201\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "import neptune\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import time\n",
        "\n",
        "crops_dir = \"/content/drive/MyDrive/TKPR221/crops/\"\n",
        "dataset_csv = \"/content/drive/MyDrive/TKPR221/dataset.csv\"\n",
        "original_df = pd.read_csv(dataset_csv).iloc[:]\n",
        "df = original_df.groupby('label').sample(n=round(10000/399), random_state=221, replace=True)\n",
        "\n",
        "print(original_df.describe())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "image_size = 256 # 256x256\n",
        "\n",
        "transformations = A.Compose([\n",
        "    A.RandomRotate90(),\n",
        "    A.HorizontalFlip(),\n",
        "    A.VerticalFlip(),\n",
        "    A.RandomBrightnessContrast()\n",
        "])\n",
        "\n",
        "\n",
        "dataset = TrafficSignDataset(crops_dir, df, size=image_size, flatten=True, transformations=transformations)\n",
        "\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator()\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2048)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2048)\n",
        "\n",
        "\n",
        "sgd_clf = SGDClassifier(\n",
        "    loss='log_loss',\n",
        "    penalty='l1',\n",
        "    alpha=0.001,\n",
        "    max_iter=1000,\n",
        "    random_state=221\n",
        ")\n",
        "\n",
        "run = neptune.init_run(\n",
        "    project=\"muguryalcin/TKPR221\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMjhjOTdkOS03YjZiLTRlYWMtOWYzMi05MGIwNmYzZTMxZmMifQ==\",\n",
        "    name=f\"SGDClassifier_{time.strftime('%Y%m%d_%H%M%S')}\",\n",
        "    description=\"Description of the experiment\",\n",
        "    tags=[\"SGDClassifier\", \"l1_losg_loss_alpha0001\"]\n",
        ")\n",
        "\n",
        "\n",
        "run[\"hyperparameters\"] = {\n",
        "    \"n_epochs\": n_epochs,\n",
        "\n",
        "    \"model\": {\n",
        "        \"type\": \"SGDClassifier\",\n",
        "        \"loss\": sgd_clf.get_params()['loss'],\n",
        "        \"penalty\": sgd_clf.get_params()['penalty'],\n",
        "        \"alpha\": sgd_clf.get_params()['alpha'],\n",
        "        \"max_iter\": sgd_clf.get_params()['max_iter']\n",
        "    },\n",
        "\n",
        "    \"dataset\": {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"total_size\": len(dataset),\n",
        "        \"train_size\": train_size,\n",
        "        \"val_size\": val_size,\n",
        "        \"test_size\": test_size,\n",
        "        \"n_classes\": len(dataset.get_label_encoded_classes()),\n",
        "        \"image_size\": image_size,\n",
        "        \"flattened\": True,\n",
        "        \"transformations\": transformations\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYtukv9q0BDZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "all_classes = dataset.get_label_encoded_classes()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loop = tqdm(train_loader, desc=f'Training Epoch {epoch+1} / {n_epochs}')\n",
        "    for batch_X, batch_y in train_loop:\n",
        "        X = batch_X.numpy()\n",
        "        y = batch_y.numpy()\n",
        "        sgd_clf.partial_fit(X, y, classes=all_classes)\n",
        "\n",
        "\n",
        "    train_preds = sgd_clf.predict(X)\n",
        "\n",
        "    run[\"train/accuracy\"].append(accuracy_score(y, train_preds))\n",
        "    run[\"train/f1\"].append(f1_score(y, train_preds, average='weighted'))\n",
        "    run[\"train/recall\"].append(recall_score(y, train_preds, average='weighted'))\n",
        "    run[\"train/precision\"].append(precision_score(y, train_preds, average='weighted'))\n",
        "    run[\"train/loss\"].append(sgd_clf.get_params()['loss'])\n",
        "\n",
        "\n",
        "    val_preds = []\n",
        "    val_true = []\n",
        "    for batch_X, batch_y in val_loader:\n",
        "        X = batch_X.numpy()\n",
        "        val_preds.extend(sgd_clf.predict(X))\n",
        "        val_true.extend(batch_y.numpy())\n",
        "\n",
        "    val_acc = accuracy_score(val_true, val_preds)\n",
        "    val_f1 = f1_score(val_true, val_preds, average='weighted')\n",
        "    val_recall = recall_score(val_true, val_preds, average='weighted')\n",
        "    val_precision = precision_score(val_true, val_preds, average='weighted')\n",
        "\n",
        "    print(f\"Epoch {epoch+1} validation metrics:\")\n",
        "    print(f\"Accuracy: {val_acc:.4f}\")\n",
        "    print(f\"F1 Score: {val_f1:.4f}\")\n",
        "    print(f\"Recall: {val_recall:.4f}\")\n",
        "    print(f\"Precision: {val_precision:.4f}\")\n",
        "\n",
        "\n",
        "    run[\"val/accuracy\"].append(val_acc)\n",
        "    run[\"val/f1\"].append(val_f1)\n",
        "    run[\"val/recall\"].append(val_recall)\n",
        "    run[\"val/precision\"].append(val_precision)\n",
        "\n",
        "\n",
        "test_preds = []\n",
        "test_true = []\n",
        "for batch_X, batch_y in test_loader:\n",
        "    X = batch_X.numpy()\n",
        "    test_preds.extend(sgd_clf.predict(X))\n",
        "    test_true.extend(batch_y.numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_true, test_preds)\n",
        "test_f1 = f1_score(test_true, test_preds, average='weighted')\n",
        "test_recall = recall_score(test_true, test_preds, average='weighted')\n",
        "test_precision = precision_score(test_true, test_preds, average='weighted')\n",
        "\n",
        "print(f\"\\nFinal test metrics:\")\n",
        "print(f\"Accuracy: {test_acc:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "\n",
        "\n",
        "run[\"test/accuracy\"].append(test_acc)\n",
        "run[\"test/f1\"].append(test_f1)\n",
        "run[\"test/recall\"].append(test_recall)\n",
        "run[\"test/precision\"].append(test_precision)\n",
        "\n",
        "run.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Remarks: My group and I wrote the majority of this code snippet, and ChatGPT was primarily utilized for debugging.  "
      ],
      "metadata": {
        "id": "ihASAWbHzqR4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}